{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unyR_usnLaS5"
      },
      "source": [
        "**Lab 10**\n",
        "\n",
        "Name: \n",
        "\n",
        "Date: \n",
        "\n",
        "**As you work through the code below, respond to all in-line comments and questions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pje7zNr1Logn"
      },
      "source": [
        "#Functional API for Model Construction\n",
        "\n",
        "Let's build a model for the [CIFAR10 dataset](https://keras.io/api/datasets/cifar10/).  This is a dataset that Keras can readily access.  It contains 50,000 small, color images from 10 different classes. \n",
        "\n",
        "We will start by pulling in some data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCEvdP8pMhbu"
      },
      "source": [
        "import numpy as np\n",
        "from keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, BatchNormalization, Dropout, SeparableConv2D, GlobalAveragePooling2D, concatenate\n",
        "from keras.models import Sequential, Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "\n",
        "train_X = train_X.astype('float32') / 255\n",
        "test_X = test_X.astype('float32') / 255\n",
        "\n",
        "train_Y = to_categorical(train_Y)\n",
        "test_Y = to_categorical(test_Y)\n",
        "\n",
        "print(train_X[0,:])\n",
        "print(train_Y[0,:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aovsiA7M4qT"
      },
      "source": [
        "For the first model, we will implemented a simple CNN using the [functional API](https://keras.io/guides/functional_api/) from Keras.  When we fit the model, we specify two callbacks (i.e., [early stopping](https://keras.io/api/callbacks/early_stopping/), [model checkpointing](https://keras.io/api/callbacks/model_checkpoint/)).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gqpBLiqNNoe"
      },
      "source": [
        "# --- Model 1 ---\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIRPtQ6COM1K"
      },
      "source": [
        "For the second model, let's add some [batch normalization](https://keras.io/api/layers/normalization_layers/batch_normalization/)). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNulRWi3PC7I"
      },
      "source": [
        "# --- Model 2 ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgz0Z4U5PFfD"
      },
      "source": [
        "For the third model, we will make use of [separable 2d convolutions](https://keras.io/api/layers/convolution_layers/separable_convolution2d/).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVlOQ0LFPz_0"
      },
      "source": [
        "# --- Model 3 ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YDpuW0hP5m_"
      },
      "source": [
        "Now create two of your own models.  You might consider using concatenate to create an model with an [inception block](https://machinelearningmastery.com/how-to-implement-major-architecture-innovations-for-convolutional-neural-networks/).  Add your model architectures to the boneyard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzEJjaRxRLr-"
      },
      "source": [
        "# --- Model 4 ---\n",
        "\n",
        "# --- Model 5 ---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNH82lh5RVwm"
      },
      "source": [
        "Now let's evaluate the models.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxmY4ZrYR8go"
      },
      "source": [
        "\n",
        "def calc_accuracy(x, y):\n",
        "    idxs=np.argmax(x,axis=1)\n",
        "    p = np.zeros(x.shape)\n",
        "    for i, idx in enumerate(idxs):\n",
        "        p[i,idx] = 1\n",
        "\n",
        "    return np.sum(np.multiply(p, y)) / x.shape[0]\n",
        "\n",
        "# Evaluate all models\n",
        "del model_1\n",
        "del model_2\n",
        "del model_3\n",
        "del model_4\n",
        "del model_5\n",
        "\n",
        "model_1 = load_model('model_1.h5')\n",
        "model_2 = load_model('model_2.h5')\n",
        "model_3 = load_model('model_3.h5')\n",
        "model_4 = load_model('model_4.h5')\n",
        "model_5 = load_model('model_5.h5')\n",
        "\n",
        "test_loss_1, test_acc_1 = model_1.evaluate(test_x, test_y)\n",
        "test_loss_2, test_acc_2 = model_2.evaluate(test_x, test_y)\n",
        "test_loss_3, test_acc_3 = model_3.evaluate(test_x, test_y)\n",
        "test_loss_4, test_acc_4 = model_4.evaluate(test_x, test_y)\n",
        "test_loss_5, test_acc_5 = model_5.evaluate(test_x, test_y)\n",
        "\n",
        "print(\"#1 \", test_loss_1, test_acc_1)\n",
        "print(\"#2 \", test_loss_2, test_acc_2)\n",
        "print(\"#3 \", test_loss_3, test_acc_3)\n",
        "print(\"#4 \", test_loss_4, test_acc_4)\n",
        "print(\"#5 \", test_loss_5, test_acc_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QAhu4xmSa3m"
      },
      "source": [
        "Finally, consider an ensemble approach.  Combine the results together and evaulate.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_zFmlJ8SlSe"
      },
      "source": [
        "# Results of ensemble predictor "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}