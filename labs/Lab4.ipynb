{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-b-aWz472C",
        "colab_type": "text"
      },
      "source": [
        "# Lab 4\n",
        "\n",
        "Name:\n",
        "\n",
        "Date: \n",
        "\n",
        "**As you work through the code below, respond to all in-line comments and questions.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRRGE5n5kXxX",
        "colab_type": "text"
      },
      "source": [
        "The first task is to build a simple sentiment classifer to label movie reviews as positive or negative.  The [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/) is a benchmark dataset that is accessible through Keras ([see imdb.load_data for full details](https://keras.io/api/datasets/imdb/)).  The feature data is a list of integers indicating what words were in a movie review.  The indices are in order of decreasing frequency (i.e., the word represented by '1' is the most common word in the corpus).  The labels are 0 or 1.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fje7UNgkmex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "# Have a look at the data ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xiVsdIbm9Tw",
        "colab_type": "text"
      },
      "source": [
        "We can put these indices directly into the network so we need to consider how we can encode these values into a feature vector.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAf1F--9nSic",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimensions=NUM_WORDS):\n",
        "    results = np.zeros((len(sequences), dimensions))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "train_X = vectorize_sequences(train_data)\n",
        "test_X = vectorize_sequences(test_data)\n",
        "\n",
        "train_Y = train_labels\n",
        "test_Y = test_labels\n",
        "\n",
        "# Take another peek at the data and labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syhwIuLOoz0o",
        "colab_type": "text"
      },
      "source": [
        "Now train up a model and evaluate it on the test dataset.  Try some different combinations of number of layers, [optimizers](https://keras.io/api/optimizers/), epochs, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu8vXpHwoy0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C78VDYW0qEOP",
        "colab_type": "text"
      },
      "source": [
        "Did you notice any relationship between the number of epochs and difference between the performance on the training and test dataset.  If so, what was it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN95zNUvsILF",
        "colab_type": "text"
      },
      "source": [
        "Now let's go back and pull of a validation dataset from training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsygWFsnqRbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = vectorize_sequences(train_data)\n",
        "test_X = vectorize_sequences(test_data)\n",
        "\n",
        "train_Y = train_labels\n",
        "test_Y = test_labels\n",
        "\n",
        "N = int(.8 * train_X.shape[0])\n",
        "val_X = train_X[N:,:]\n",
        "val_Y = train_Y[N:]\n",
        "train_X = train_X[:N,:]\n",
        "train_Y = train_Y[:N]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKxVN6x4ssF5",
        "colab_type": "text"
      },
      "source": [
        "Train up some more models but use the validation data when fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myx0aSigs1cx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ...\n",
        "\n",
        "history = model.fit(train_X, train_Y, epochs=40, batch_size=512, validation_data=(val_X, val_Y))\n",
        "\n",
        "# ...\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "print(history_dict.keys())\n",
        "\n",
        "train_acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "plt.plot(epochs, train_acc, 'bo', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7mhtqw8tI2b",
        "colab_type": "text"
      },
      "source": [
        "Now evaluate your model one the test data.  How does it perform?  Is the performance more comparable to the training data or validation data?  When we tune our model architecture, what should guide us?  How does this effect our interpretation of the model's performance on the training data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCm4aVC83ka",
        "colab_type": "text"
      },
      "source": [
        "Let's adjust the architecture so that we can predict one of multiple categories.  The Reuters dataset contains several thousand newswires that belong to one of 46 categories and is easily accessible in Keras ([see reuters.load_data for more details](https://https://keras.io/api/datasets/reuters/)).  The input to the network is similiar to the IMDB dataset.  This time we will need a target vector as well.  This will be done through one bit hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyxXNYR-_CxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_WORDS=10000\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "# Have a quick look at the training data and lables\n",
        "\n",
        "N = .8\n",
        "\n",
        "# Our vectorized training data\n",
        "train_X = vectorize_sequences(train_data[:int(train_data.shape[0] * N)])\n",
        "val_X = vectorize_sequences(train_data[int(train_data.shape[0] * N):])\n",
        "\n",
        "# Our vectorized test data\n",
        "test_X = vectorize_sequences(test_data)\n",
        "\n",
        "# Our vectorized training labels\n",
        "train_Y = to_one_hot(train_labels[:int(train_labels.shape[0] * N)])\n",
        "val_Y = to_one_hot(train_labels[int(train_labels.shape[0] * N):])\n",
        "\n",
        "# Our vectorized test labels\n",
        "test_Y = to_one_hot(test_labels)\n",
        "\n",
        "# Have a look at some of the feature vectors and targets\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqP5x3CA_rqB",
        "colab_type": "text"
      },
      "source": [
        "Now train up and evaluate some models.  Note that you will need to change the size of the output layer and loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ep7vhkt_2jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "\n",
        "#model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "#model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzXLh9oO5O0I",
        "colab_type": "text"
      },
      "source": [
        "**Submission:** Submit this notebook and a PDF of its generated output via Blackboard by the end of the day.  Submit the files individually.\n",
        "\n",
        "\n"
      ]
    }
  ]
}